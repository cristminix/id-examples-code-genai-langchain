{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb4737a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tentu! Ini beberapa lelucon tentang bola lampu:\n",
      "\n",
      "---\n",
      "\n",
      "**1. Lelucon Klasik (Psikolog)**\n",
      "\n",
      "*   **Q:** Berapa banyak psikolog yang dibutuhkan untuk mengganti bola lampu?\n",
      "*   **A:** Hanya satu, tapi bola lampunya harus *mau* berubah.\n",
      "\n",
      "---\n",
      "\n",
      "**2. Lelucon (Insinyur)**\n",
      "\n",
      "*   **Q:** Berapa banyak insinyur yang dibutuhkan untuk mengganti bola lampu?\n",
      "*   **A:** Satu, tapi pertama-tama dia harus mendesain ulang *fitting*-nya, menghitung output lumen optimal, dan mengoptimalkan efisiensi energi.\n",
      "\n",
      "---\n",
      "\n",
      "**3. Lelucon (Politisi)**\n",
      "\n",
      "*   **Q:** Berapa banyak politisi yang dibutuhkan untuk mengganti bola lampu?\n",
      "*   **A:** Tidak ada. Mereka hanya akan berjanji untuk menggantinya pada pemilihan berikutnya.\n",
      "\n",
      "---\n",
      "\n",
      "**4. Lelucon (Mahasiswa)**\n",
      "\n",
      "*   **Q:** Berapa banyak mahasiswa yang dibutuhkan untuk mengganti bola lampu?\n",
      "*   **A:** Lima. Satu yang mengganti, dan empat lainnya mengeluh betapa tidak adilnya mereka harus mengganti bola lampu.\n",
      "\n",
      "---\n",
      "\n",
      "**5. Lelucon Ringan**\n",
      "\n",
      "*   **Q:** Kenapa bola lampu dapat promosi?\n",
      "*   **A:** Karena dia sangat... *terang* (pintar)!\n",
      "\n",
      "---\n",
      "\n",
      "Semoga kamu suka! ðŸ˜„\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "# Inisialisasi model OpenAI\n",
    "# openai_llm = OpenAI()\n",
    "# Inisialisasi model Gemini\n",
    "gemini_pro = GoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "# Salah satu atau keduanya dapat digunakan dengan antarmuka yang sama\n",
    "response = gemini_pro.invoke(\"Ceritakan lelucon tentang bola lampu!\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e9022b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import FakeListLLM\n",
    "# Buat LLM palsu yang selalu mengembalikan respons yang sama\n",
    "fake_llm = FakeListLLM(responses=[\"Hello\"])\n",
    "result = fake_llm.invoke(\"Input apa pun akan mengembalikan Hello\")\n",
    "print(result)  # Output: Hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50ded0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def factorial(n):\n",
      "    \"\"\"Menghitung faktorial dari n (n!), dimana n adalah bilangan bulat non-negatif.\n",
      "    \n",
      "    Args:\n",
      "        n (int): Bilangan bulat non-negatif.\n",
      "    \n",
      "    Returns:\n",
      "        int: Faktorial dari n.\n",
      "        \n",
      "    Raises:\n",
      "        ValueError: Jika n negatif.\n",
      "    \"\"\"\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Faktorial tidak terdefinisi untuk bilangan negatif\")\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    result = 1\n",
      "    for i in range(1, n + 1):\n",
      "        result *= i\n",
      "    return result\n",
      "```\n",
      "\n",
      "### Contoh penggunaan:\n",
      "- `factorial(5)` returns `120` (karena 5! = 1Ã—2Ã—3Ã—4Ã—5 = 120)\n",
      "- `factorial(0)` returns `1`\n",
      "- `factorial(-1)` raises `ValueError`\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "chat = ChatAnthropic(model=\"claude-3-opus-20240229\")\n",
    "messages = [\n",
    "    SystemMessage(content=\"Anda adalah asisten pemrograman yang membantu\"),\n",
    "    HumanMessage(content=\"Tulis fungsi Python untuk menghitung faktorial\")\n",
    "]\n",
    "response = chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e823ae83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Algoritma QuickSelect untuk Menemukan Elemen Terbesar ke-k\n",
      "\n",
      "Untuk menemukan elemen terbesar ke-k dalam array tidak terurut dengan kompleksitas waktu optimal, saya akan menyarankan algoritma **QuickSelect**, yang merupakan variasi dari algoritma **QuickSort**. Algoritma ini bekerja dengan cara memilah (partisi) array secara rekursif berdasarkan pivot yang dipilih, dan hanya fokus pada partisi yang berisi elemen terbesar ke-k. Ini memungkinkan waktu eksekusi rata-rata linier tanpa perlu menyortir seluruh array.\n",
      "\n",
      "#### Prinsip Kerja\n",
      "- Array memiliki ukuran n.\n",
      "- Kami ingin elemen terbesar ke-k, yaitu elemen yang berada pada posisi n-k jika array diurutkan secara menurun (misalnya, elemen paling besar adalah k=1, kedua terbesar adalah k=2, dll.).\n",
      "- Gunakan indeks berbasis 0: Elemen terbesar ke-k akan menjadi elemen pada indeks n-k setelah pengurutan menurun.\n",
      "- Setiap iterasi:\n",
      "  1. Pilih pivot (acak untuk menghindari worst case).\n",
      "  2. Partisi array sehingga elemen-elemen lebih besar dari pivot di kiri, dan lebih kecil di kanan (untuk terbesar ke-k, sesuaikan partisi).\n",
      "  3. Rekursi pada subarray yang sesuai berdasarkan posisi k.\n",
      "\n",
      "Pseudocode (dalam bahasa seperti C++/Python):\n",
      "\n",
      "```\n",
      "int kthLargest(vector<int>& A, int k) {\n",
      "    int n = A.size();\n",
      "    return quickSelect(A, 0, n-1, n-k);  // n-k untuk kth largest (karena indeks 0-based)\n",
      "}\n",
      "\n",
      "int quickSelect(vector<int>& A, int left, int right, int target) {\n",
      "    // Pilih pivot acak untuk menghindari worst case\n",
      "    int pivotIdx = left + rand() % (right - left + 1);\n",
      "    swap(A[left], A[pivotIdx]);\n",
      "    \n",
      "    // Partisi: elemen >= pivot di kiri, < pivot di kanan\n",
      "    int pivot = A[left];\n",
      "    int i = left + 1;\n",
      "    for (int j = left + 1; j <= right; j++) {\n",
      "        if (A[j] >= pivot) {  // Untuk largest, pastikan pivot besar di kiri\n",
      "            swap(A[i], A[j]);\n",
      "            i++;\n",
      "        }\n",
      "    }\n",
      "    swap(A[left], A[i-1]);  // Pivot di posisi terakhir i-1\n",
      "    int pivotPos = i - 1;\n",
      "    \n",
      "    // Jika posisi target, return\n",
      "    if (pivotPos == target) return A[pivotPos];\n",
      "    // Rekursi\n",
      "    else if (pivotPos > target) return quickSelect(A, left, pivotPos-1, target);\n",
      "    else return quickSelect(A, pivotPos+1, right, target);\n",
      "}\n",
      "```\n",
      "\n",
      "**Catatan**: \n",
      "- Partisi disesuaikan agar elemen lebih besar atau sama dengan pivot berada di kiri untuk fokus pada terbesar ke-k.\n",
      "- Pivoting acak memastikan kompleksitas rata-rata optimal.\n",
      "- Jika k berada di luar range (mis. k > n), kembalikan error.\n",
      "\n",
      "#### Analisis Kompleksitas\n",
      "- **Waktu**:\n",
      "  - Rata-rata: O(n). Seperti QuickSort, setiap partisi membagi array menjadi dua bagian. Dengan pivot acak, probabilitas partisi tidak seimbang rendah, sehingga total kerja adalah O(n) untuk n elemen. Ini dibuktikan melalui analisis ekspektasi.\n",
      "  - Worst-case: O(nÂ²) jika pivot selalu buruk (mis. array sudah terurut), namun dengan pivoting acak, ini jarang terjadi dan dapat dihindari menggunakan teknik seperti median-of-medians untuk membuatnya O(n) terjamin tanpa random.\n",
      "- **Ruang**:\n",
      "  - O(log n) rata-rata untuk stack rekursi (kedalaman rekursi maksimal log n pada partisi seimbang).\n",
      "  - O(1) tambahan jika menggunakan partisi in-place tanpa ruang ekstra.\n",
      "\n",
      "#### Mengapa Optimal?\n",
      "Algoritma ini optimal dalam kompleksitas waktu rata-rata karena:\n",
      "- Lower bound teoritis: Dalam kasus terburuk, algoritma seleksi apa pun (seperti ini) membutuhkan Î©(n) waktu untuk memeriksa semua elemen, karena kita tidak tahu elemen mana yang terbesar tanpa membandingkannya. QuickSelect mencapai batas ini dengan O(n) ekspektasi, menjadikannya optimal secara praktis dan teoritis untuk masalah ini.\n",
      "- Dibandingkan alternatif lain:\n",
      "  - Sorting penuh (mis. QuickSort atau MergeSort): O(n log n), yang lebih buruk dari O(n).\n",
      "  - Heap-based: Menggunakan max-heap untuk ekstraksi ke-k elemen terbesar membutuhkan O(n log k), yang lebih baik untuk k kecil tapi tidak linier untuk k umum.\n",
      "  - Dengan QuickSelect, kita hanya memproses subset elemen yang relevan, mencapai waktu linier maksimal.\n",
      "\n",
      "Ini adalah solusi standar dan efisien untuk masalah ini dalam praktik, terutama dengan input besar. Jika Anda perlu versi deterministik (tanpa random) dengan O(n) terjamin, gunakan Median of Medians, yang lebih kompleks tapi aman.\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# Buat template\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Anda adalah programmer berpengalaman dan analis matematika.\"),\n",
    "    (\"user\", \"{problem}\")\n",
    "])\n",
    "# Inisialisasi Claude dengan pemikiran diperluas diaktifkan\n",
    "chat = ChatAnthropic(\n",
    "    model_name=\"claude-3-7-sonnet-20240326\",  # Gunakan versi model terbaru\n",
    "    max_tokens=64_000,                        # Batas panjang respons total\n",
    "    thinking={\"type\": \"enabled\", \"budget_tokens\": 15000},  # Alokasikan token untuk berpikir\n",
    ")\n",
    "# Buat dan jalankan rantai\n",
    "chain = template | chat\n",
    "# Masalah algoritmik kompleks\n",
    "problem = \"\"\"\n",
    "Rancang algoritma untuk menemukan elemen terbesar keâ€‘k dalam array tidak terurut\n",
    "dengan kompleksitas waktu optimal. Analisis kompleksitas waktu dan ruang\n",
    "solusi Anda dan jelaskan mengapa itu optimal.\n",
    "\"\"\"\n",
    "# Dapatkan respons dengan pemikiran disertakan\n",
    "response = chat.invoke([HumanMessage(content=problem)])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14947201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Strategi Optimal untuk Menjadi Kaya dalam Rupiah\n",
      "\n",
      "Menjadi kaya dalam rupiah (mata uang Indonesia) bukanlah tentang keberuntungan semata, melainkan kombinasi disiplin, pendidikan, dan keputusan strategis. Saya akan menghitung atau merinci strategi optimal berdasarkan prinsip-prinsip keuangan yang terbukti, seperti yang digunakan dalam investasi, penghematan, dan pengembangan aset. Ini bukan jaminan, karena risiko seperti inflasi, fluktuasi ekonomi, dan kondisi pasar selalu ada. Mari kita pecah menjadi langkah-langkah praktis dengan estimasi perhitungan sederhana.\n",
      "\n",
      "#### 1. **Asesmen Awal: Hitung Posisi Keuangan Anda**\n",
      "   - **Langkah**: Hitung pendapatan bulanan, pengeluaran, dan tabungan saat ini. Gunakan rumus dasar: **Tabungan Bersih = Pendapatan - Pengeluaran**.\n",
      "   - **Contoh Perhitungan**: Jika pendapatan Anda Rp10 juta/bulan dan pengeluaran Rp8 juta, maka tabungan Rp2 juta/bulan. Tujuan: Capai rasio tabungan minimal 20-30% dari pendapatan.\n",
      "   - **Strategi Optimal**: Jika tabungan rendah, fokuskan pada pengurangan pengeluaran tidak penting (misalnya, hindari belanja impulsif). Target: Dalam 1 tahun, akumulasi tabungan Rp24-36 juta.\n",
      "\n",
      "#### 2. **Bangun Dana Darurat dan Kurangi Utang**\n",
      "   - **Langkah**: Simpan dana darurat setara dengan 3-6 bulan pengeluaran (misalnya, Rp30-60 juta jika pengeluaran Rp10 juta/bulan). Kurangi utang berbunga tinggi (kartu kredit, pinjaman konsumtif).\n",
      "   - **Perhitungan**: Jika utang bunga 15%/tahun, bayar minimum + tambahan untuk menghemat bunga. Contoh: Utang Rp10 juta dengan bunga Rp1,5 juta/tahun â†’ Bayar ekstra Rp500.000/bulan untuk lunas dalam 2 tahun, hemat Rp1 juta.\n",
      "   - **Strategi Optimal**: Prioritaskan utang berdasarkan metode debt avalanche (bunga tertinggi dulu). Ini membebaskan arus kas untuk investasi.\n",
      "\n",
      "#### \n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Anda adalah asisten pemecahan masalah.\"),\n",
    "    (\"user\", \"{problem}\")\n",
    "])\n",
    "# Inisialisasi dengan parameter reasoning_effort\n",
    "chat = ChatOpenAI(\n",
    "    model=\"vscodeser,gpt-4o\",\n",
    "    reasoning_effort=\"low\",  # Opsi: \"low\", \"medium\", \"high\",\n",
    "    temperature=0.8, top_p=0.95, max_tokens=512\n",
    ")\n",
    "chain = template | chat\n",
    "response = chain.invoke({\"problem\": \"Hitung strategi optimal untuk kaya dalam rupiah\"})\n",
    "# chat = ChatOpenAI(model=\"vscodeser,gpt-4o\")\n",
    "# chain = template | chat\n",
    "# response = chain.invoke({\"problem\": \"Hitung strategi optimal untuk...\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aed17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(question, context=None):\n",
    "    if context:\n",
    "        return f\"Informasi konteks: {context}\\n\\nJawab pertanyaan ini dengan singkat: {question}\"\n",
    "    return f\"Jawab pertanyaan ini dengan singkat: {question}\"\n",
    "\n",
    "# contoh penggunaan:\n",
    "prompt_text = generate_prompt(\"Apa ibu kota Prancis?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d271e1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jawab pertanyaan ini dengan singkat: Apa ibu kota Prancis?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "# Tentukan sekali, gunakan di mana saja\n",
    "question_template = PromptTemplate.from_template( \"Jawab pertanyaan ini dengan singkat: {question}\" )\n",
    "question_with_context_template = PromptTemplate.from_template( \"Informasi konteks: {context}\\n\\nJawab pertanyaan ini dengan singkat: {question}\" )\n",
    "# Hasilkan prompt dengan mengisi variabel\n",
    "prompt_text = question_template.format(question=\"Apa ibu kota Prancis?\")\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90148a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Salut, Ã§a va ?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 507, 'prompt_tokens': 233, 'total_tokens': 740, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 501, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 192, 'video_tokens': 0}, 'cost': 3.8627e-05, 'is_byok': True, 'cost_details': {'upstream_inference_cost': 0.00077254, 'upstream_inference_prompt_cost': 1.204e-05, 'upstream_inference_completions_cost': 0.0007605}}, 'model_provider': 'openai', 'model_name': 'x-ai/grok-code-fast-1', 'system_fingerprint': None, 'id': 'gen-1767421907-tZ0Q36SGrT511X8guHH5', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019b828e-0e5f-74e3-ad54-347f3581f895-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 233, 'output_tokens': 507, 'total_tokens': 740, 'input_token_details': {'audio': 0, 'cache_read': 192}, 'output_token_details': {'reasoning': 501}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Anda adalah penerjemah Inggris ke Prancis.\"),\n",
    "    (\"user\", \"Terjemahkan ini ke Prancis: {text}\")\n",
    "])\n",
    "chat = ChatOpenAI(  model=\"vscodeser,gpt-4o\",reasoning_effort=\"low\")\n",
    "formatted_messages = template.format_messages(text=\"Halo, apa kabar?\")\n",
    "response = chat.invoke(formatted_messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a9ccf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_chain = prompt | llm | StrOutputParser()\n",
    "chain = RunnableSequence(first= prompt, middle=[llm], last= output_parser) \n",
    "with_transformation = prompt | llm | (lambda x: x.upper()) | StrOutputParser()\n",
    "\n",
    "decision_chain = prompt | llm | (lambda x: route_based_on_content(x)) | {\n",
    "    \"summarize\": summarize_chain,\n",
    "    \"analyze\": analyze_chain\n",
    "}\n",
    "\n",
    "# Fungsi ke Runnable\n",
    "length_func = lambda x: len(x)\n",
    "chain = prompt | length_func | output_parser\n",
    "# Dikonversi menjadi:\n",
    "chain = prompt | RunnableLambda(length_func) | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a54f4fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lelucon klasik Programmer:\n",
      "\n",
      "Dua programer lagi ngobrol di kafe. Yang satu bilang: \"Ada masalah apa hari ini?\" Yang lain jawab: \"Aku lagi debug kode, tapi selalu ada exception yang tak terduga. Kayaknya aku butuh null pointer untuk ngefix ini.\"\n",
      "\n",
      "Kemudian dia bilang kalah: \"Exception? Ya, aku juga sering nemu NullPointerException. Siapa tau itu error atau romantis alias 'Null Pointer Romance'?\"\n",
      "\n",
      "Atau yang lebih simpel: Kenapa programmer takut dengan abad ke-21? Karena Y2K bug!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "# Buat komponen\n",
    "prompt = PromptTemplate.from_template(\"Ceritakan lelucon tentang {topic}\")\n",
    "llm = ChatOpenAI(model=\"vscodeser,gpt-4o\")\n",
    "output_parser = StrOutputParser()\n",
    "# Rantai bersama menggunakan LCEL\n",
    "chain = prompt | llm | output_parser\n",
    "#  Eksekusi alur kerja dengan satu panggilan\n",
    "result = chain.invoke({\"topic\": \"pemrograman\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90fb7848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kenapa programmer takut dengan HTML?\n",
      "\n",
      "Karena mereka tahu bahwa tag `<body>` bisa di-bully kapan saja! ðŸ˜‰\n"
     ]
    }
   ],
   "source": [
    "# Tanpa LCEL, alur kerja yang sama setara dengan panggilan fungsi terpisah \n",
    "# dengan penerusan data manual:\n",
    "\n",
    "formatted_prompt = prompt.invoke({\"topic\": \"pemrograman\"})\n",
    "llm_output = llm.invoke(formatted_prompt)\n",
    "result = output_parser.invoke(llm_output)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67f23bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analisis: Suasana cerita ini didominasi oleh nuansa **ketenangan, kenyamanan, introspeksi, dan kedamaian yang mendalam**. Hujan, yang seringkali diasosiasikan dengan melankoli atau kesedihan, justru digambarkan sebagai elemen kunci yang menciptakan suasana positif ini.\n",
      "\n",
      "Berikut adalah analisis lebih lanjut:\n",
      "\n",
      "1.  **Kenyamanan dan Kehangatan (Cozy and Warmth):**\n",
      "    *   **Sensorik:** Suara rintik hujan yang *lembut*, selimut *hangat*, cahaya *kelabu lembut yang menenangkan*, secangkir kopi *hangat* dengan uap mengepul, sofa *favorit*, selimut rajutan *tebal*, dan sepiring pisang goreng *hangat*.\n",
      "    *   **Efek:** Detail-detail ini secara konsisten membangun perasaan nyaman, terlindung, dan manja di dalam rumah, kontras dengan dunia luar yang basah dan dingin. Ini menciptakan semacam \"sarang\" pribadi bagi Maya.\n",
      "\n",
      "2.  **Ketenangan dan Perlambatan Waktu (Tranquility and Slowing Down):**\n",
      "    *   **Alasan:** Ini hari Sabtu, tidak ada rutinitas kerja, dan Maya menikmati \"sensasi malas\". Waktu digambarkan \"merangkak perlahan, tanpa beban jam atau tenggat waktu.\"\n",
      "    *   **Efek:** Suasana ini mengundang pembaca untuk merasakan jeda dari hiruk pikuk kehidupan sehari-hari. Ritme hujan yang \"berirama seperti musik pengantar tidur\" semakin menegaskan kedamaian dan ketenangan.\n",
      "\n",
      "3.  **Introspeksi dan Refleksi Positif (Introspection and Positive Reflection):**\n",
      "    *   **Perasaan Maya:** Maya tidak menganggap hari hujan sebagai halangan, melainkan \"hadiah berupa waktu yang diperlambat, kesempatan untuk introspeksi, dan menikmati kedamaian sederhana di rumah.\" Ia membiarkan suara hujan \"memeluknya,\" dan berharap bangun dengan \"semangat yang diperbarui.\"\n",
      "    *   **Efek:** Suasana ini mendorong perenungan diri, penghargaan terhadap momen-momen kecil, dan transformasi pandangan terhadap hal-hal yang biasanya dianggap negatif (hujan) menjadi positif.\n",
      "\n",
      "4.  **Estetika Visual dan Sensorik (Visual and Sensory Aesthetics):**\n",
      "    *   **Visual:** \"Cahaya pagi memudar menjadi kelabu lembut\", \"dunia di luar tampak seperti lukisan cat air\", \"pepohonan basah\", \"jalan berkilauan\", \"cahaya temaram\" dari lampu meja kecil menciptakan nuansa puitis, lembut, dan sedikit melankolis (dalam arti artistik, bukan sedih) yang mendukung suasana reflektif.\n",
      "    *   **Bau:** \"Aroma tanah basah dan kesegaran alam\" serta \"aroma manis pisang bercampur dengan wangi kopi\" menambahkan dimensi sensorik yang kaya, menarik pembaca lebih dalam ke pengalaman Maya.\n",
      "\n",
      "Secara keseluruhan, suasana cerita adalah **oase ketenangan dan kebahagiaan sederhana**. Ini adalah sebuah perayaan akan keindahan momen-momen sunyi, di mana alam (hujan) menjadi katalisator bagi kedamaian internal, kenyamanan fisik, dan peremajaan jiwa. Tidak ada ketegangan, konflik, atau kesedihan yang berarti; yang ada hanyalah apresiasi mendalam terhadap \"hari hujan yang indah ini.\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# Inisialisasi model\n",
    "llm = GoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "# Rantai pertama menghasilkan cerita\n",
    "story_prompt = PromptTemplate.from_template(\"Tulis cerita pendek tentang {topic}\")\n",
    "story_chain = story_prompt | llm | StrOutputParser()\n",
    "# Rantai kedua menganalisis cerita\n",
    "analysis_prompt = PromptTemplate.from_template(\n",
    "    \"Analisis suasana cerita berikut:\\n{story}\"\n",
    ")\n",
    "analysis_chain = analysis_prompt | llm | StrOutputParser()\n",
    "# Gabungkan rantai\n",
    "story_with_analysis = story_chain | analysis_chain\n",
    "# Jalankan rantai gabungan\n",
    "story_analysis = story_with_analysis.invoke({\"topic\": \"hari hujan\"})\n",
    "print(\"\\nAnalisis:\", story_analysis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "id-examples-code-genai-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
